import os
import json
import time
import threading
import requests
import re
from flask import Flask, request, jsonify
from flask_cors import CORS
from bs4 import BeautifulSoup
import firebase_admin
from firebase_admin import credentials, firestore
from pymongo import MongoClient
import certifi
from datetime import datetime

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# ==========================================
app = Flask(__name__)
CORS(app)

# Ù…ÙØªØ§Ø­ Ø³Ø±ÙŠ Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø·
API_SECRET = os.environ.get('API_SECRET', 'Zeusndndjddnejdjdjdejekk29393838msmskxcm9239484jdndjdnddjj99292938338zeuslojdnejxxmejj82283849')

# ==========================================
# Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================

# 1. MongoDB Setup
MONGO_URI = os.environ.get('MONGODB_URI')
novels_collection = None
if MONGO_URI:
    try:
        mongo_client = MongoClient(MONGO_URI, tlsCAFile=certifi.where())
        mongo_db = mongo_client['zeus'] 
        novels_collection = mongo_db['novels']
        print("âœ… Connected to MongoDB")
    except Exception as e:
        print(f"âŒ MongoDB Connection Error: {e}")
else:
    print("âš ï¸ MONGODB_URI not found in env vars")

# 2. Firebase Setup
FIREBASE_KEY = os.environ.get('FIREBASE_SERVICE_ACCOUNT')
firestore_db = None
if FIREBASE_KEY:
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ø´Ø§Ù…Ù„
        firebase_key_cleaned = FIREBASE_KEY.strip()
        # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ BOM Ø£Ùˆ Ø£Ø­Ø±Ù Ø®ÙÙŠØ©
        firebase_key_cleaned = firebase_key_cleaned.encode('utf-8').decode('utf-8-sig')
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ JSON
        cred_dict = json.loads(firebase_key_cleaned)
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…ÙØªØ§Ø­ Ø§Ù„Ø®Ø§Øµ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚
        if 'private_key' in cred_dict:
            private_key = cred_dict['private_key']
            private_key = private_key.replace('\\n', '\n')
            lines = private_key.split('\n')
            cleaned_lines = []
            for line in lines:
                if '-----BEGIN' in line or '-----END' in line:
                    cleaned_lines.append(line.strip())
                else:
                    cleaned_line = line.strip().replace(' ', '').replace('\t', '')
                    if cleaned_line:
                        cleaned_lines.append(cleaned_line)
            cred_dict['private_key'] = '\n'.join(cleaned_lines)
        
        # ØªÙ‡ÙŠØ¦Ø© Firebase
        if not firebase_admin._apps:
            cred = credentials.Certificate(cred_dict)
            firebase_admin.initialize_app(cred)
        firestore_db = firestore.client()
        print("âœ… Connected to Firebase Firestore")
        
    except Exception as e:
        print(f"âŒ Firebase Connection Error: {e}")
else:
    print("âš ï¸ FIREBASE_SERVICE_ACCOUNT not found in env vars")

# ==========================================
# Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø³Ø­Ø¨ (Scraper Tools)
# ==========================================

def get_headers():
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ar,en-US;q=0.7,en;q=0.3'
    }

def get_slug_from_url(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±Ù Ø§Ù„ÙØ±ÙŠØ¯ Ù„Ù„Ø±ÙˆØ§ÙŠØ© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    try:
        parts = url.rstrip('/').split('/novel/')
        if len(parts) > 1:
            return parts[1].split('/')[0]
    except:
        pass
    return None

def extract_background_image(style_str):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ø³ØªØ§ÙŠÙ„ background-image"""
    if not style_str: return ''
    match = re.search(r'url\(&quot;(.*?)&quot;\)', style_str)
    if not match:
        match = re.search(r'url\("(.*?)"\)', style_str)
    if not match:
        match = re.search(r'url\((.*?)\)', style_str)
    return match.group(1) if match else ''

def fetch_novel_metadata_html(url):
    """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø±ÙˆØ§ÙŠØ© Ù…Ù† HTML Ø§Ù„ØµÙØ­Ø© Ù…Ø¨Ø§Ø´Ø±Ø©"""
    try:
        print(f"ğŸ“¡ Fetching metadata from HTML: {url}")
        response = requests.get(url, headers=get_headers(), timeout=15)
        if response.status_code != 200:
            print(f"âŒ HTTP Error: {response.status_code}")
            return None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 1. Title
        title_tag = soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else "Unknown Title"
        
        # 2. Cover
        cover_url = ""
        img_div = soup.find('div', class_='v-image__image--cover')
        if img_div and img_div.has_attr('style'):
            cover_url = extract_background_image(img_div['style'])
            
        # 3. Description
        desc_div = soup.find(class_='text-pre-line') or soup.find('div', class_='v-card__text')
        description = desc_div.get_text(strip=True) if desc_div else ""
        
        # 4. Status & Category
        chips = soup.find_all(class_='v-chip__content')
        status = "Ù…Ø³ØªÙ…Ø±Ø©"
        tags = []
        category = "Ø¹Ø§Ù…"
        
        for chip in chips:
            text = chip.get_text(strip=True)
            if text in ['Ù…ÙƒØªÙ…Ù„Ø©', 'Ù…ØªÙˆÙ‚ÙØ©', 'Ù…Ø³ØªÙ…Ø±Ø©']:
                status = text
            elif text not in ['Ù…ØªØ±Ø¬Ù…Ø©', 'Ø±ÙˆØ§ÙŠØ©']: 
                tags.append(text)
        
        if tags:
            category = tags[0]

        # 5. Total Chapters
        total_chapters = 0
        tabs = soup.find_all(class_='v-tab')
        for tab in tabs:
            tab_text = tab.get_text(strip=True)
            if "Ø§Ù„ÙØµÙˆÙ„" in tab_text:
                match = re.search(r'\((\d+)\)', tab_text)
                if match:
                    total_chapters = int(match.group(1))
                    break
        
        return {
            'title': title,
            'description': description,
            'cover': cover_url,
            'status': status,
            'tags': tags,
            'category': category,
            'total_chapters': total_chapters
        }

    except Exception as e:
        print(f"âŒ Error scraping metadata: {e}")
        return None

def scrape_chapter_content_html(novel_url, chapter_num):
    """Ø³Ø­Ø¨ Ù†Øµ Ø§Ù„ÙØµÙ„ Ù…Ø¹ ØªØ¬Ø±Ø¨Ø© ÙƒÙ„Ø§Ø³Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©"""
    url = f"{novel_url.rstrip('/')}/{chapter_num}"
    try:
        response = requests.get(url, headers=get_headers(), timeout=10)
        if response.status_code != 200:
            return None, None
            
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ø¹Ø¯Ø© Ø·Ø±Ù‚ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø§Ù„ÙØ´Ù„
        content_div = soup.find('div', class_='pre-formatted') or \
                      soup.find('div', class_='v-card__text') or \
                      soup.find('div', class_='chapter-content') or \
                      soup.find('div', id='chapter-article')
            
        if content_div:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ø£ÙˆØ³Ù…Ø© ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
            for extra in content_div.find_all(['script', 'style', 'ins']):
                extra.decompose()
                
            paragraphs = content_div.find_all(['p', 'div'])
            if paragraphs:
                text_content = "\n\n".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
            else:
                text_content = content_div.get_text(separator="\n\n", strip=True)
            
            # ÙÙ„ØªØ± Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Øµ Ø­Ù‚ÙŠÙ‚ÙŠ
            if len(text_content.strip()) < 50:
                return None, None
        else:
            return None, None

        title_tag = soup.find(class_='v-card__subtitle') or soup.find('h1')
        title = title_tag.get_text(strip=True) if title_tag else f"Ø§Ù„ÙØµÙ„ {chapter_num}"
        title = re.sub(r'^\d+\s*-\s*', '', title)

        return title, text_content
            
    except Exception as e:
        print(f"Error scraping chapter {chapter_num}: {e}")
        return None, None

def background_worker(url, admin_email, author_name):
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„ØªÙŠ ØªØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©"""
    print(f"ğŸš€ Starting HTML scraper for: {url}")
    
    # 1. Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
    metadata = fetch_novel_metadata_html(url)
    if not metadata:
        print("âŒ Failed to fetch metadata from HTML")
        return

    print(f"ğŸ“– Found Novel: {metadata['title']} ({metadata['total_chapters']} Chapters)")

    # 2. Ø¥Ù†Ø´Ø§Ø¡ Ø£Ùˆ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø±ÙˆØ§ÙŠØ© ÙÙŠ MongoDB ÙÙˆØ±Ø§Ù‹ Ù„ÙƒÙŠ ØªØ¸Ù‡Ø± ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
    novel_id = None
    if novels_collection is not None:
        existing_novel = novels_collection.find_one({'title': metadata['title'], 'authorEmail': admin_email})
        
        novel_doc = {
            'title': metadata['title'],
            'description': metadata['description'],
            'cover': metadata['cover'],
            'author': author_name,
            'authorEmail': admin_email,
            'category': metadata['category'],
            'tags': metadata['tags'],
            'status': metadata['status'],
            'sourceUrl': url,
            'lastChapterUpdate': datetime.now()
        }

        if existing_novel:
            novel_id = existing_novel['_id']
            novels_collection.update_one({'_id': novel_id}, {'$set': novel_doc})
            print(f"ğŸ”„ Novel updated in MongoDB: {novel_id}")
        else:
            novel_doc['createdAt'] = datetime.now()
            novel_doc['chapters'] = []
            novel_doc['views'] = 0
            result = novels_collection.insert_one(novel_doc)
            novel_id = result.inserted_id
            print(f"ğŸ†• New novel created in MongoDB: {novel_id}")
    else:
        print("âŒ MongoDB not connected, cannot proceed.")
        return

    # 3. Ø­Ù„Ù‚Ø© Ø³Ø­Ø¨ Ø§Ù„ÙØµÙˆÙ„
    total = metadata['total_chapters']
    if total == 0:
        print("âš ï¸ No chapters count found, trying first 100 blind...")
        total = 100

    # Ø¬Ù„Ø¨ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙØµÙˆÙ„ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø­Ø§Ù„ÙŠØ§Ù‹
    current_novel = novels_collection.find_one({'_id': novel_id})
    existing_numbers = [c['number'] for c in current_novel.get('chapters', [])] if current_novel else []

    for num in range(1, total + 1):
        if num in existing_numbers:
            print(f"â© Skipping Ch {num} (Exists)")
            continue

        chap_title, content = scrape_chapter_content_html(url, num)
        
        if content:
            try:
                # Ø£) Ø§Ù„Ø­ÙØ¸ ÙÙŠ Firebase (Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ)
                if firestore_db is not None:
                    doc_ref = firestore_db.collection('novels').document(str(novel_id)).collection('chapters').document(str(num))
                    doc_ref.set({
                        'title': chap_title,
                        'content': content,
                        'lastUpdated': firestore.SERVER_TIMESTAMP
                    })

                # Ø¨) Ø§Ù„ØªØ­Ø¯ÙŠØ« ÙÙŠ MongoDB (Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙØµÙˆÙ„)
                if novels_collection is not None:
                    chapter_meta = {
                        'number': num,
                        'title': chap_title,
                        'createdAt': datetime.now(),
                        'views': 0
                    }
                    novels_collection.update_one(
                        {'_id': novel_id},
                        {'$push': {'chapters': chapter_meta}}
                    )
                
                print(f"âœ… Chapter {num} uploaded successfully.")
                time.sleep(1.5)
                
            except Exception as e:
                print(f"âŒ DB Save Error Ch {num}: {e}")
        else:
            print(f"âš ï¸ Failed to scrape content for Ch {num}")

    print("âœ¨ Scraping Task Completed Successfully!")

# ==========================================
# Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (Endpoints)
# ==========================================

@app.route('/', methods=['GET'])
def health_check():
    return "ZEUS HTML Scraper Service is Running âš¡ v2.1", 200

@app.route('/scrape', methods=['POST'])
def trigger_scrape():
    auth_header = request.headers.get('Authorization')
    if auth_header != API_SECRET:
        return jsonify({'message': 'Unauthorized'}), 401

    data = request.json
    url = data.get('url')
    admin_email = data.get('adminEmail')
    author_name = data.get('authorName', 'ZEUS Bot')

    if not url or 'rewayat.club' not in url:
        return jsonify({'message': 'Invalid URL. Must be from rewayat.club'}), 400

    # Ø¨Ø¯Ø¡ Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
    thread = threading.Thread(target=background_worker, args=(url, admin_email, author_name))
    thread.daemon = True 
    thread.start()

    return jsonify({
        'message': 'ØªÙ… Ø¨Ø¯Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. Ø§Ù„Ø±ÙˆØ§ÙŠØ© Ø³ØªØ¸Ù‡Ø± ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ùƒ Ø®Ù„Ø§Ù„ Ø«ÙˆØ§Ù†Ù ÙˆÙŠØ¨Ø¯Ø£ Ø³Ø­Ø¨ Ø§Ù„ÙØµÙˆÙ„.',
        'status': 'started'
    }), 200

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port)
